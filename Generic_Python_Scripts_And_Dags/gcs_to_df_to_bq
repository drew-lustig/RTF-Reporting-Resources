from datetime import date, timedelta, datetime
import logging
from google.cloud import bigquery as bq
from googleapiclient import discovery
from google.oauth2 import service_account
import pandas as pd
import pandas as pd
from google.oauth2 import service_account
from googleapiclient.discovery import build
import re


from dateutil.relativedelta import relativedelta

import os
import sys
from datetime import datetime, timedelta, date
import sys
if sys.version_info[0] < 3:
    from StringIO import StringIO
else:
    from io import StringIO
import numpy as np


#Don't edit any of this, it is good the way it is.

from google.oauth2 import service_account
import json
import base64
from google.cloud import kms_v1
from google.cloud import storage as gcsStorage

SCOPE_TABLE='rtf_global_spend_report.looker_to_bq' #scope table example is Global_Goals.Consolidate.
PROJECT_ID='essence-analytics-dwh'

def fetch_blob(bucket_name, source_blob_name):
    """Downloads a blob from the bucket."""
    storage_client = gcsStorage.Client()
    bucket = storage_client.get_bucket(bucket_name)
    blob = bucket.blob(source_blob_name)
    blob_data=blob.download_as_string()
    return blob_data

def decrypt_symmetric(encrypted_text):
    """Decrypts input ciphertext using the provided symmetric CryptoKey."""

    # crypto key params
    project_id='essence-analytics-etl-207615'
    location_id='global'
    key_ring_id='TeamAnalytics'
    crypto_key_id='SecureAnalytics'
    # Creates an API client for the KMS API.
    client = kms_v1.KeyManagementServiceClient()
    response=''
    # The resource name of the CryptoKey.
    name = client.crypto_key_path_path(project_id, location_id, key_ring_id,
                                       crypto_key_id)
    # Use the KMS API to decrypt the data.

    ciphertext=base64.b64decode(encrypted_text)
    response = json.loads(client.decrypt(name, ciphertext).plaintext)
    return response

# Use This function to fetch your credentials. Don't store credentials. Keys could be deleted anytime due to security issue.
# You should have logic to regenrate credentials if you encounter google.auth.exceptions.RefreshError
def getCredentialsFromVault(service_account_email):
    StorageBucket='essence-analytics-5a9706e0-d22d-40f3-abde-2a996137bb0f'
    blob_name=service_account_email+'.json.enc'
    enc_key=fetch_blob(StorageBucket, blob_name)
    credentials = service_account.Credentials.from_service_account_info(decrypt_symmetric(enc_key))
    print(credentials)
    return credentials


def storeDFtoBigQuery(df,table):
    df.rename(columns=lambda x: x.strip(), inplace=True)
    df.rename(columns=lambda x: re.sub('[^a-zA-Z0-9]', '_', x), inplace=True)
    df.rename(columns=lambda x: re.sub('__*', '_', x), inplace=True)
    df.columns = df.columns.str.replace(' ', '_')
    df.columns = df.columns.str.replace('-', '')
    df.columns = df.columns.str.replace('.', '')
    df.columns = df.columns.str.replace('/', '_')
    df.columns = df.columns.str.replace('__', '_')

    df.to_gbq(destination_table= table,project_id= PROJECT_ID,if_exists='replace') #fail/replace/append

ggs = fetch_blob('sixty-analytics','rtf_google_global_spend_refresh')
s=str(ggs,'utf-8')

data = StringIO(s) 

df=pd.read_csv(data)

df

#Scope, this one is good for everything-spreadsheets (reading and writing), google drive and bigquery.

API_SCOPES = ['https://www.googleapis.com/auth/spreadsheets',
          'https://spreadsheets.google.com/feeds',
          'https://www.googleapis.com/auth/drive',
          'https://www.googleapis.com/auth/cloud-platform']

#service e-mail.
service_account_email='data-strategy@essence-analytics-dwh.iam.gserviceaccount.com'

#On Google sheets, share this e-mail on the google sheets page you want to pull data from.
credentialsFromVault=getCredentialsFromVault(service_account_email)
credentialsFromVault = credentialsFromVault.with_scopes(API_SCOPES)
df['Performance And Delivery 3. Spend in Client currency'] = df['Performance And Delivery 3. Spend in Client currency'].astype(float)
df['Time 2. Month of Activity'] = df['Time 2. Month of Activity'].astype(int)
storeDFtoBigQuery(df,SCOPE_TABLE)

